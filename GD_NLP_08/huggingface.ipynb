{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d35d2da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaaceb5e026546d1bff6d8fb48f5d6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets \n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertForSequenceClassification, DataCollatorWithPadding\n",
    "import torch\n",
    "model = BertForSequenceClassification.from_pretrained(\"klue/bert-base\",num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "\n",
    "dataset = load_dataset(\"nsmc\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53264695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 9976970\n",
      "document : 아 더빙.. 진짜 짜증나네요 목소리\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 3819312\n",
      "document : 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "label : 1\n",
      "\n",
      "\n",
      "id : 10265843\n",
      "document : 너무재밓었다그래서보는것을추천한다\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 9045019\n",
      "document : 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 6483659\n",
      "document : 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n",
      "label : 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = dataset['train']\n",
    "cols = train.column_names\n",
    "cols\n",
    "\n",
    "for i in range(5):\n",
    "    for col in cols:\n",
    "        print(col, \":\", train[col][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd0c1e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    return tokenizer(\n",
    "        data['document'],\n",
    "#         data['sentence2'],\n",
    "        truncation = True,\n",
    "#         padding = 'max_length',\n",
    "        return_token_type_ids = False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8450b6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-a288ec65e207023b.arrow\n",
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-a447321391324fec.arrow\n"
     ]
    }
   ],
   "source": [
    "hf_dataset = dataset.map(transform, batched=True)\n",
    "\n",
    "# train & validation & test split\n",
    "hf_train_dataset = hf_dataset['train']\n",
    "# hf_val_dataset = hf_dataset['validation']\n",
    "hf_test_dataset = hf_dataset['test']\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cd1dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train_dataset = hf_train_dataset.remove_columns([\"document\", \"id\"])\n",
    "hf_test_dataset = hf_test_dataset.remove_columns([\"document\", \"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48f11166",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset = hf_train_dataset.select([0, 10, 20, 30, 40, 50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f91784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-ca1f1d1686438807.arrow and /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-ffcce9497da87073.arrow\n"
     ]
    }
   ],
   "source": [
    "tt = hf_train_dataset.train_test_split(test_size=0.05)\n",
    "train_dataset = tt[\"train\"]\n",
    "validation_dataset = tt[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56d7162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"steps\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 64,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 32,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "#     weight_decay = 0.01,                        # weight decay\n",
    "#     label_names=[\"label\"],\n",
    "    fp16=True,\n",
    "    group_by_length =True,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",  # Save the best model checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "283908a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    predictions = pred.predictions.argmax(axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, pos_label=1)\n",
    "    recall = recall_score(labels, predictions, pos_label=1)\n",
    "    f1 = f1_score(labels, predictions, pos_label=1)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc8cb44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "804abe3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running training *****\n",
      "  Num examples = 142500\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6681\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6681' max='6681' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6681/6681 23:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.333500</td>\n",
       "      <td>0.284925</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0.855391</td>\n",
       "      <td>0.901940</td>\n",
       "      <td>0.878049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.278500</td>\n",
       "      <td>0.256574</td>\n",
       "      <td>0.892400</td>\n",
       "      <td>0.891193</td>\n",
       "      <td>0.891433</td>\n",
       "      <td>0.891313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.248743</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>0.904474</td>\n",
       "      <td>0.887662</td>\n",
       "      <td>0.895989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.246400</td>\n",
       "      <td>0.241884</td>\n",
       "      <td>0.898400</td>\n",
       "      <td>0.886530</td>\n",
       "      <td>0.911369</td>\n",
       "      <td>0.898778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>0.241563</td>\n",
       "      <td>0.902667</td>\n",
       "      <td>0.889295</td>\n",
       "      <td>0.917565</td>\n",
       "      <td>0.903209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.189000</td>\n",
       "      <td>0.250840</td>\n",
       "      <td>0.902667</td>\n",
       "      <td>0.908717</td>\n",
       "      <td>0.893050</td>\n",
       "      <td>0.900815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.189800</td>\n",
       "      <td>0.246981</td>\n",
       "      <td>0.902000</td>\n",
       "      <td>0.888744</td>\n",
       "      <td>0.916756</td>\n",
       "      <td>0.902533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.185800</td>\n",
       "      <td>0.240441</td>\n",
       "      <td>0.905600</td>\n",
       "      <td>0.904416</td>\n",
       "      <td>0.904903</td>\n",
       "      <td>0.904659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.174700</td>\n",
       "      <td>0.263167</td>\n",
       "      <td>0.904267</td>\n",
       "      <td>0.894987</td>\n",
       "      <td>0.913793</td>\n",
       "      <td>0.904292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.266477</td>\n",
       "      <td>0.904533</td>\n",
       "      <td>0.900535</td>\n",
       "      <td>0.907328</td>\n",
       "      <td>0.903918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.275348</td>\n",
       "      <td>0.907467</td>\n",
       "      <td>0.902615</td>\n",
       "      <td>0.911369</td>\n",
       "      <td>0.906971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.261151</td>\n",
       "      <td>0.905200</td>\n",
       "      <td>0.895806</td>\n",
       "      <td>0.914871</td>\n",
       "      <td>0.905238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>0.265432</td>\n",
       "      <td>0.906933</td>\n",
       "      <td>0.901011</td>\n",
       "      <td>0.912177</td>\n",
       "      <td>0.906560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7500\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7500\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7500\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2227\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2227/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2227/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7500\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7500\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7500\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7500\n",
      "  Batch size = 32\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4454\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4454/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4454/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-6500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7500\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7500\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7500\n",
      "  Batch size = 32\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7500\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7500\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-6681\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-6681/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-6681/pytorch_model.bin\n",
      "Deleting older checkpoint [/aiffel/aiffel/transformers/checkpoint-2227] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6681, training_loss=0.19905552794843748, metrics={'train_runtime': 1407.3977, 'train_samples_per_second': 303.752, 'train_steps_per_second': 4.747, 'total_flos': 5183265179485680.0, 'train_loss': 0.19905552794843748, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=train_dataset,    # training dataset\n",
    "    eval_dataset=validation_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35c0b861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 03:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.27031320333480835,\n",
       " 'eval_accuracy': 0.90704,\n",
       " 'eval_precision': 0.9028775566285872,\n",
       " 'eval_recall': 0.9136376276168912,\n",
       " 'eval_f1': 0.9082257236504364,\n",
       " 'eval_runtime': 222.143,\n",
       " 'eval_samples_per_second': 225.08,\n",
       " 'eval_steps_per_second': 7.036,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.evaluate(hf_test_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
