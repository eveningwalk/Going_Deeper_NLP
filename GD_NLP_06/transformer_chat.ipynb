{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca9a0f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "from konlpy.tag import Mecab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44187ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c24cfebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d3a4bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d421cc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44dc16f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "662cca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc49f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eee00b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7970eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f2f9e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0baba1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6347e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbd05f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b28f18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SD카드 망가졌어</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SD카드 안돼</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SNS 맞팔 왜 안하지ㅠㅠ</td>\n",
       "      <td>잘 모르고 있을 수도 있어요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SNS 시간낭비인 거 아는데 매일 하는 중</td>\n",
       "      <td>시간을 정하고 해보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SNS 시간낭비인데 자꾸 보게됨</td>\n",
       "      <td>시간을 정하고 해보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SNS보면 나만 빼고 다 행복해보여</td>\n",
       "      <td>자랑하는 자리니까요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>가끔 궁금해</td>\n",
       "      <td>그 사람도 그럴 거예요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>가끔 뭐하는지 궁금해</td>\n",
       "      <td>그 사람도 그럴 거예요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>가끔은 혼자인게 좋다</td>\n",
       "      <td>혼자를 즐기세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>가난한 자의 설움</td>\n",
       "      <td>돈은 다시 들어올 거예요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>가만 있어도 땀난다</td>\n",
       "      <td>땀을 식혀주세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>가상화폐 쫄딱 망함</td>\n",
       "      <td>어서 잊고 새출발 하세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>가스불 켜고 나갔어</td>\n",
       "      <td>빨리 집에 돌아가서 끄고 나오세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>가스불 켜놓고 나온거 같아</td>\n",
       "      <td>빨리 집에 돌아가서 끄고 나오세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>가스비 너무 많이 나왔다.</td>\n",
       "      <td>다음 달에는 더 절약해봐요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>가스비 비싼데 감기 걸리겠어</td>\n",
       "      <td>따뜻하게 사세요!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>가스비 장난 아님</td>\n",
       "      <td>다음 달에는 더 절약해봐요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>가장 확실한 건 뭘까?</td>\n",
       "      <td>가장 확실한 시간은 오늘이에요. 어제와 내일을 놓고 고민하느라 시간을 낭비하지 마세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>가족 여행 가기로 했어</td>\n",
       "      <td>온 가족이 모두 마음에 드는 곳으로 가보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>가족 여행 고고</td>\n",
       "      <td>온 가족이 모두 마음에 드는 곳으로 가보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>가족 여행 어디로 가지?</td>\n",
       "      <td>온 가족이 모두 마음에 드는 곳으로 가보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>가족 있어?</td>\n",
       "      <td>저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>가족관계 알려 줘</td>\n",
       "      <td>저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>가족끼리 여행간다.</td>\n",
       "      <td>더 가까워질 기회가 되겠네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>가족들 보고 싶어</td>\n",
       "      <td>저도요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Q                                                 A  \\\n",
       "0                    12시 땡!                                        하루가 또 가네요.   \n",
       "1               1지망 학교 떨어졌어                                         위로해 드립니다.   \n",
       "2              3박4일 놀러가고 싶다                                       여행은 언제나 좋죠.   \n",
       "3           3박4일 정도 놀러가고 싶다                                       여행은 언제나 좋죠.   \n",
       "4                   PPL 심하네                                        눈살이 찌푸려지죠.   \n",
       "5                 SD카드 망가졌어                                다시 새로 사는 게 마음 편해요.   \n",
       "6                   SD카드 안돼                                다시 새로 사는 게 마음 편해요.   \n",
       "7            SNS 맞팔 왜 안하지ㅠㅠ                                  잘 모르고 있을 수도 있어요.   \n",
       "8   SNS 시간낭비인 거 아는데 매일 하는 중                                     시간을 정하고 해보세요.   \n",
       "9         SNS 시간낭비인데 자꾸 보게됨                                     시간을 정하고 해보세요.   \n",
       "10      SNS보면 나만 빼고 다 행복해보여                                       자랑하는 자리니까요.   \n",
       "11                   가끔 궁금해                                     그 사람도 그럴 거예요.   \n",
       "12              가끔 뭐하는지 궁금해                                     그 사람도 그럴 거예요.   \n",
       "13              가끔은 혼자인게 좋다                                         혼자를 즐기세요.   \n",
       "14                가난한 자의 설움                                    돈은 다시 들어올 거예요.   \n",
       "15               가만 있어도 땀난다                                         땀을 식혀주세요.   \n",
       "16               가상화폐 쫄딱 망함                                    어서 잊고 새출발 하세요.   \n",
       "17               가스불 켜고 나갔어                               빨리 집에 돌아가서 끄고 나오세요.   \n",
       "18           가스불 켜놓고 나온거 같아                               빨리 집에 돌아가서 끄고 나오세요.   \n",
       "19           가스비 너무 많이 나왔다.                                   다음 달에는 더 절약해봐요.   \n",
       "20          가스비 비싼데 감기 걸리겠어                                         따뜻하게 사세요!   \n",
       "21                가스비 장난 아님                                   다음 달에는 더 절약해봐요.   \n",
       "22             가장 확실한 건 뭘까?  가장 확실한 시간은 오늘이에요. 어제와 내일을 놓고 고민하느라 시간을 낭비하지 마세요.   \n",
       "23             가족 여행 가기로 했어                         온 가족이 모두 마음에 드는 곳으로 가보세요.   \n",
       "24                 가족 여행 고고                         온 가족이 모두 마음에 드는 곳으로 가보세요.   \n",
       "25            가족 여행 어디로 가지?                         온 가족이 모두 마음에 드는 곳으로 가보세요.   \n",
       "26                   가족 있어?     저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요   \n",
       "27                가족관계 알려 줘     저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요   \n",
       "28               가족끼리 여행간다.                                  더 가까워질 기회가 되겠네요.   \n",
       "29                가족들 보고 싶어                                              저도요.   \n",
       "\n",
       "    label  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "5       0  \n",
       "6       0  \n",
       "7       0  \n",
       "8       0  \n",
       "9       0  \n",
       "10      0  \n",
       "11      0  \n",
       "12      0  \n",
       "13      0  \n",
       "14      0  \n",
       "15      0  \n",
       "16      0  \n",
       "17      0  \n",
       "18      0  \n",
       "19      0  \n",
       "20      0  \n",
       "21      0  \n",
       "22      0  \n",
       "23      0  \n",
       "24      0  \n",
       "25      0  \n",
       "26      0  \n",
       "27      0  \n",
       "28      0  \n",
       "29      0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://github.com/songys/Chatbot_data\n",
    "train_data = pd.read_csv('~/aiffel/transformer_chatbot/data/ChatbotData.csv')\n",
    "train_data.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "450fa54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q        0\n",
      "A        0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da349c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "  # 입력받은 sentence를 소문자로 변경하고 양쪽 공백을 제거\n",
    "  sentence = sentence.lower()\n",
    "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "  # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
    "  # student와 온점 사이에 거리를 만듭니다.\n",
    "  sentence = re.sub(r\"[^a-zA-Z?.!,가-힣ㄱ-ㅎㅏ-ㅣ0-9]+\", \" \", sentence)  \n",
    "  sentence = sentence.strip()\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3a4dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 전처리하고 토큰화하는 함수\n",
    "def build_corpus(data, tokenizer):\n",
    "    train_data = data.dropna(how = 'any') \n",
    "    train_data.drop_duplicates(subset=['Q'], inplace=True)#Q중복제거\n",
    "    train_data.drop_duplicates(subset=['A'], inplace=True)\n",
    "    que_corpus=[]\n",
    "    ans_corpus=[]\n",
    "    max_len = 50  # 일정 길이 이상인 문장은 제외\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        src_sentence = preprocess_sentence(row['Q'])\n",
    "        tgt_sentence = preprocess_sentence(row['A'])\n",
    "\n",
    "        src_tokens = tokenizer.morphs(src_sentence)\n",
    "        tgt_tokens = tokenizer.morphs(tgt_sentence)\n",
    "\n",
    "        if len(src_tokens) <= 0 or len(tgt_tokens) <= 0:\n",
    "            continue  # 토큰의 개수가 0인 문장은 제외\n",
    "\n",
    "        if len(src_tokens) > max_len or len(tgt_tokens) > max_len:\n",
    "            continue  # 일정 길이 이상인 문장은 제외\n",
    "        que_corpus.append(src_tokens)\n",
    "        ans_corpus.append([\"<start>\"] + tgt_tokens + [\"<end>\"])\n",
    "    words = np.concatenate(que_corpus+ans_corpus).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common()\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "    return que_corpus,ans_corpus,word_to_index, {index:word for word, index in word_to_index.items()},len(vocab)\n",
    "mecab = Mecab()\n",
    "\n",
    "    \n",
    "que_corpus, ans_corpus, word_to_index, index_to_word, VOCAB_SIZE = build_corpus(train_data, mecab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3855cab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11823\n",
      "전체 샘플 수 : 11823\n",
      "전처리 후의 22번째 질문 샘플: ['가스', '비', '장난', '아님']\n",
      "전처리 후의 22번째 답변 샘플: ['<start>', '다음', '달', '에', '는', '더', '절약', '해', '봐요', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 로드하고 전처리하여 질문을 questions, 답변을 answers에 저장합니다.\n",
    "print('전체 샘플 수 :', len(que_corpus))\n",
    "print('전체 샘플 수 :', len(ans_corpus))\n",
    "print('전처리 후의 22번째 질문 샘플: {}'.format(que_corpus[21]))\n",
    "print('전처리 후의 22번째 답변 샘플: {}'.format(ans_corpus[21]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dff1308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence) \n",
    "#     return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "519629e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_count = len(que_corpus)//200\n",
    "\n",
    "enc_train = get_encoded_sentences(que_corpus[:-test_sentence_count], word_to_index)\n",
    "dec_train = get_encoded_sentences(ans_corpus[:-test_sentence_count], word_to_index)\n",
    "\n",
    "enc_test = get_encoded_sentences(que_corpus[-test_sentence_count:], word_to_index)\n",
    "dec_test = get_encoded_sentences(ans_corpus[-test_sentence_count:], word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0710bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "enc_ndarray = tf.keras.preprocessing.sequence.pad_sequences(enc_train, maxlen=MAX_LEN, padding='post')\n",
    "dec_ndarray = tf.keras.preprocessing.sequence.pad_sequences(dec_train, maxlen=MAX_LEN, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad36002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_ndarray, dec_ndarray)).batch(batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57ea3bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=d_model,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb9a47dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92087c9e84304b2db7b7d8d52ffa680c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1158be1a644264ad099845f9319279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bcc752d33884816836a004b63235a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8facea5f9e574794b7d6907f8a960390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db2d904a2bf439ca80c536202978c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6cc787fccd4df1ab85fa80ff5f97aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e11bf678454af19bd6889dd98c706f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7210dcf5983d4265a04141cb6f6fb35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2ede6385794d9f88e2c92f15e88f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b033ef99462a400b9366aacc5f6e5f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Q. 위의 코드를 활용하여 모델을 훈련시켜봅시다!\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "    for step, (enc_batch, dec_batch) in enumerate(train_dataset):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_batch,\n",
    "                    dec_batch,\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        tqdm_bar.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        tqdm_bar.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (step + 1)))\n",
    "        tqdm_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8c25551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ba74878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(tokens, model, src_tokenizer, tgt_tokenizer):\n",
    "    padded_tokens = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=MAX_LEN,\n",
    "                                                           padding='post')\n",
    "    ids = []\n",
    "    output = tf.expand_dims([word_to_index['<start>']], 0)   \n",
    "    for i in range(MAX_LEN):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(padded_tokens, output)\n",
    "\n",
    "        predictions, _, _, _ = model(padded_tokens, \n",
    "                                      output,\n",
    "                                      enc_padding_mask,\n",
    "                                      combined_mask,\n",
    "                                      dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if word_to_index['<end>'] == predicted_id:\n",
    "#             result = tgt_tokenizer.decode_ids(ids)  \n",
    "            print(ids)\n",
    "            print([word_to_index['<start>']])\n",
    "            result = ids\n",
    "            return result\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "#     result = tgt_tokenizer.decode_ids(ids)  \n",
    "    result = ids  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b1709fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "#     src_tokens = src_tokenizer.encode_as_ids(src_sentence)\n",
    "#     tgt_tokens = tgt_tokenizer.encode_as_ids(tgt_sentence)\n",
    "\n",
    "    if (len(src_sentence) > MAX_LEN): return None\n",
    "    if (len(tgt_sentence) > MAX_LEN): return None\n",
    "\n",
    "#     reference = tgt_sentence.split()\n",
    "    candidate = translate(src_sentence, model, src_tokenizer, tgt_tokenizer)#.split()\n",
    "\n",
    "    score = sentence_bleu([tgt_sentence], candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03198fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(model, src_sentences, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx], src_tokenizer, tgt_tokenizer, verbose)\n",
    "        if not score: continue\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d7847df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179c00de278e43e29af1c33b9f95fdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 24, 8, 268, 15, 24, 55, 16, 30, 4]\n",
      "[5]\n",
      "[15, 24, 8, 268, 94, 14, 62, 16, 30, 4]\n",
      "[5]\n",
      "[37, 20, 102, 42, 25, 18, 21, 54, 9, 95, 195, 106, 102, 24, 19, 11, 4]\n",
      "[5]\n",
      "[623, 414, 37, 20, 102, 42, 18, 9, 18, 10, 16, 30, 4]\n",
      "[5]\n",
      "[214, 214, 1815, 42, 18, 554, 62, 42, 18, 31, 4]\n",
      "[5]\n",
      "[15, 24, 8, 9, 551, 7, 79, 89, 189, 165, 13, 19, 9, 95, 276, 93, 4]\n",
      "[5]\n",
      "[15, 24, 8, 9, 95, 276, 93, 4]\n",
      "[5]\n",
      "[15, 24, 8, 9, 56, 10, 499, 9, 26, 15, 27, 38, 4]\n",
      "[5]\n",
      "[15, 24, 8, 14, 18, 9, 95, 276, 93, 4]\n",
      "[5]\n",
      "[15, 24, 8, 9, 95, 115, 7, 59, 66, 23, 47, 4]\n",
      "[5]\n",
      "[65, 193, 8, 21, 54, 89, 23, 47, 4]\n",
      "[5]\n",
      "[134, 7, 44, 9, 193, 59, 9, 95, 276, 93, 4]\n",
      "[5]\n",
      "[37, 7, 38, 4]\n",
      "[5]\n",
      "[15, 24, 8, 76, 201, 4]\n",
      "[5]\n",
      "[37, 48, 369, 28, 456, 491, 12, 93, 4]\n",
      "[5]\n",
      "[15, 24, 8, 9, 56, 7, 59, 66, 23, 47, 4]\n",
      "[5]\n",
      "[15, 24, 8, 9, 95, 732, 214, 61, 79, 24, 19, 9, 95, 276, 93, 4]\n",
      "[5]\n",
      "[804, 81, 944, 14, 18, 9, 233, 348, 7, 143, 16, 30, 4]\n",
      "[5]\n",
      "[15, 24, 8, 9, 95, 109, 145, 50, 8, 9, 26, 15, 110, 4]\n",
      "[5]\n",
      "[37, 7, 44, 9, 16, 145, 269, 31, 4]\n",
      "[5]\n",
      "[247, 21, 54, 89, 23, 200, 4]\n",
      "[5]\n",
      "[419, 650, 9, 95, 44, 14, 292, 7, 23, 47, 4]\n",
      "[5]\n",
      "[83, 86, 68, 20, 99, 86, 1078, 171, 650, 26, 19, 11, 4]\n",
      "[5]\n",
      "[1530, 1690, 19, 11, 4]\n",
      "[5]\n",
      "[56, 1661, 342, 441, 68, 8, 9, 26, 15, 27, 36, 4]\n",
      "[5]\n",
      "[234, 86, 193, 55, 16, 30, 4]\n",
      "[5]\n",
      "[292, 7, 80, 4]\n",
      "[5]\n",
      "[324, 103, 111, 350, 12, 157, 8, 36, 4]\n",
      "[5]\n",
      "[311, 8, 268, 19, 39, 15, 27, 38, 4]\n",
      "[5]\n",
      "[193, 8, 542, 54, 10, 292, 7, 145, 50, 74, 4]\n",
      "[5]\n",
      "[56, 7, 82, 725, 15, 27, 38, 4]\n",
      "[5]\n",
      "[15, 997, 332, 9, 352, 12, 59, 76, 201, 4]\n",
      "[5]\n",
      "[188, 121, 188, 7, 38, 4]\n",
      "[5]\n",
      "[195, 33, 2774, 9, 126, 121, 8, 9, 95, 44, 9, 681, 7, 80, 4]\n",
      "[5]\n",
      "[132, 474, 443, 27, 128, 132, 12, 15, 27, 31, 4]\n",
      "[5]\n",
      "[37, 7, 38, 4]\n",
      "[5]\n",
      "[56, 7, 494, 8, 38, 4]\n",
      "[5]\n",
      "[324, 165, 13, 19, 9, 95, 276, 93, 4]\n",
      "[5]\n",
      "[324, 165, 13, 19, 9, 95, 276, 93, 4]\n",
      "[5]\n",
      "[367, 143, 16, 30, 4]\n",
      "[5]\n",
      "[175, 7, 15, 24, 8, 9, 551, 7, 15, 27, 31, 4]\n",
      "[5]\n",
      "[310, 86, 797, 9, 551, 7, 230, 13, 19, 9, 62, 7, 80, 4]\n",
      "[5]\n",
      "[175, 7, 778, 47, 4]\n",
      "[5]\n",
      "[174, 10, 57, 171, 164, 61, 65, 850, 8, 268, 338, 178, 362, 338, 4]\n",
      "[5]\n",
      "[476, 35, 1039, 7, 102, 16, 30, 4]\n",
      "[5]\n",
      "[195, 12, 109, 39, 15, 24, 8, 9, 95, 276, 93, 4]\n",
      "[5]\n",
      "[331, 7, 785, 185, 359, 23, 47, 4]\n",
      "[5]\n",
      "[104, 29, 7, 79, 89, 23, 47, 4]\n",
      "[5]\n",
      "[427, 348, 7, 143, 42, 18, 31, 4]\n",
      "[5]\n",
      "[56, 7, 195, 56, 7, 195, 12, 109, 145, 50, 74, 4]\n",
      "[5]\n",
      "[15, 20, 174, 7, 80, 4]\n",
      "[5]\n",
      "[7, 170, 237, 7, 170, 62, 7, 330, 407, 27, 38, 4]\n",
      "[5]\n",
      "[214, 68, 17, 19, 11, 4]\n",
      "[5]\n",
      "[193, 55, 42, 18, 10, 16, 30, 4]\n",
      "[5]\n",
      "[854, 12, 427, 348, 7, 59, 189, 50, 8, 9, 26, 15, 27, 38, 4]\n",
      "[5]\n",
      "[854, 12, 427, 348, 7, 59, 189, 50, 8, 9, 26, 15, 27, 38, 4]\n",
      "[5]\n",
      "[770, 86, 51, 60, 23, 47, 4]\n",
      "[5]\n",
      "[37, 7, 38, 4]\n",
      "[5]\n",
      "[15, 24, 404, 16, 30, 4]\n",
      "[5]\n",
      "Num of Sample: 59\n",
      "Total Score: 0.024038661123995216\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(transformer, enc_test, dec_test, mecab, mecab, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a004fbde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 78, 14, 23, 138, 211, 14, 165, 13, 19, 9, 95, 276, 93, 4]\n",
      "[5]\n",
      "마음 먹 고 나 서 놀 고 물 어 보 는 건 어떨까 요 .\n",
      "[170, 64, 614, 347, 344, 7, 36, 4]\n",
      "[5]\n",
      "참 기 위해 그게 인생 이 죠 .\n"
     ]
    }
   ],
   "source": [
    "print(get_decoded_sentence(translate(get_encoded_sentence(mecab.morphs('지루하다, 놀러가고 싶어.'), word_to_index), transformer, mecab, mecab), index_to_word))\n",
    "print(get_decoded_sentence(translate(get_encoded_sentence(mecab.morphs('오늘 일찍 일어났더니 피곤하다.'), word_to_index), transformer, mecab, mecab), index_to_word))\n",
    "\n",
    "# sentence_generation('배고파')\n",
    "# sentence_generation('졸려')\n",
    "# sentence_generation('안녕')\n",
    "# sentence_generation('내일 날씨')\n",
    "# sentence_generation('재밌는 얘기 해봐')\n",
    "# sentence_generation('입출력이 뭐야?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aee1acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "wv = Word2VecKeyedVectors.load('/aiffel/data/word2vec_ko.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2c3b621",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('드라마', 0.8418774008750916),\n",
       " ('뮤지컬', 0.7775140404701233),\n",
       " ('코미디', 0.7489107251167297),\n",
       " ('다큐멘터리', 0.7401294708251953),\n",
       " ('헐리우드', 0.7397844195365906),\n",
       " ('애니메이션', 0.7170552015304565),\n",
       " ('독립영화', 0.7113528251647949),\n",
       " ('로맨틱', 0.7107657194137573),\n",
       " ('장편', 0.7101576924324036),\n",
       " ('극영화', 0.7045413255691528)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.wv.most_similar(positive='영화', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "048c7aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "\n",
    "    res = []\n",
    "    toks = sentence\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "\n",
    "    except:   # 단어장에 없는 단어\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res.append(_to)\n",
    "        else: res.append(tok)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f10ed21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6091748d214669b9436a8535777ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5095eb0e90247fb976af1cda2eb783e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12', '시', '땡', '!\">']\n",
      "['<start>', '하루', '가', '또', '가', '네요', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "que_arg = []\n",
    "ans_arg = []\n",
    "for enc_tokens, dec_tokens in tqdm(zip(enc_train, dec_train)):\n",
    "    new_enc = lexical_sub([index_to_word[index] if index in index_to_word else '<UNK>' for index in enc_tokens], wv.wv)\n",
    "    new_dec = lexical_sub([index_to_word[index] if index in index_to_word else '<UNK>' for index in dec_tokens[1:-1]], wv.wv)\n",
    "        \n",
    "    if new_enc is not None: \n",
    "        que_arg.append(new_enc)\n",
    "        ans_arg.append([index_to_word[index] if index in index_to_word else '<UNK>' for index in dec_tokens])\n",
    "    if new_dec is not None: \n",
    "        que_arg.append([index_to_word[index] if index in index_to_word else '<UNK>' for index in enc_tokens])\n",
    "#         print(new_dec)\n",
    "        new_dec = [\"<start>\"]+new_dec+['<end>']\n",
    "        ans_arg.append(new_dec)\n",
    "    que_arg.append([index_to_word[index] if index in index_to_word else '<UNK>' for index in enc_tokens])\n",
    "    ans_arg.append([index_to_word[index] if index in index_to_word else '<UNK>' for index in dec_tokens])\n",
    "    \n",
    "for enc_tokens, dec_tokens in tqdm(zip(que_arg, ans_arg)):\n",
    "    print(enc_tokens)\n",
    "    print(dec_tokens)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3da52376",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train = get_encoded_sentences(que_arg, word_to_index)\n",
    "dec_train = get_encoded_sentences(ans_arg, word_to_index)\n",
    "enc_ndarray = tf.keras.preprocessing.sequence.pad_sequences(enc_train, maxlen=MAX_LEN, padding='post')\n",
    "dec_ndarray = tf.keras.preprocessing.sequence.pad_sequences(dec_train, maxlen=MAX_LEN, padding='post')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_ndarray, dec_ndarray)).batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51006d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=d_model,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a119d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de25d55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6f8013fdd8413fa97160a367893e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6385ee28214a4c2784c78887b251799c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d795dfba2a84995b05bacde25d15905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9a29c97b444813957a89e6a3c7e9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d60dbd294554e649937633c8af39b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe46524ba5804f679d87b362d7c38fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c857e4b9f70540c583264312a37011fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58bc9940bda4c2498545a548305cc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5c290f83304071b3258b125a030873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdea7512bfa4bb297c7acb27b13f870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "    for step, (enc_batch, dec_batch) in enumerate(train_dataset):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_batch,\n",
    "                    dec_batch,\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        tqdm_bar.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        tqdm_bar.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (step + 1)))\n",
    "        tqdm_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dbf66c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4cdb424ef146ada1c6e49f400b9d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[393, 1403, 61, 74, 4]\n",
      "[5]\n",
      "[576, 28, 165, 13, 19, 14, 40, 13, 19, 171, 15, 27, 31, 4]\n",
      "[5]\n",
      "[15, 24, 8, 9, 26, 109, 145, 1403, 84, 9, 562, 8, 14, 18, 31, 4]\n",
      "[5]\n",
      "[23, 7, 171, 650, 171, 15, 27, 12, 15, 27, 128, 15, 27, 12, 15, 27, 12, 15, 27, 12, 15, 27, 12, 570, 8, 9, 26, 15, 27, 12, 15, 27, 12, 15, 27, 12, 15, 27, 12, 15, 27, 12, 15, 27, 31, 4]\n",
      "[5]\n",
      "[114, 1030, 55, 84, 12, 109, 14, 250, 4]\n",
      "[5]\n",
      "[347, 427, 350, 7, 118, 80, 4]\n",
      "[5]\n",
      "[56, 7, 1656, 337, 47, 4]\n",
      "[5]\n",
      "[15, 24, 8, 9, 2, 15, 27, 31, 4]\n",
      "[5]\n",
      "[686, 9, 26, 44, 215, 98, 21, 54, 9, 26, 15, 27, 31, 4]\n",
      "[5]\n",
      "[104, 401, 7, 193, 12, 79, 64, 596, 7, 80, 4]\n",
      "[5]\n",
      "[677, 2387, 29, 7, 38, 4]\n",
      "[5]\n",
      "[677, 35, 134, 10, 4658, 50, 17, 57, 11, 4]\n",
      "[5]\n",
      "[92, 9, 260, 350, 7, 1447, 4]\n",
      "[5]\n",
      "[68, 10, 55, 42, 18, 128, 2018, 50, 358, 7, 80, 4]\n",
      "[5]\n",
      "[397, 103, 551, 93, 4]\n",
      "[5]\n",
      "[334, 222, 405, 8, 14, 944, 529, 1024, 11, 4]\n",
      "[5]\n",
      "[23, 85, 2, 351, 4]\n",
      "[5]\n",
      "[866, 103, 7, 32, 375, 8, 11, 4]\n",
      "[5]\n",
      "[1260, 31, 4]\n",
      "[5]\n",
      "[697, 81, 37, 810, 221, 42, 18, 31, 4]\n",
      "[5]\n",
      "[476, 35, 871, 328, 12, 75, 319, 21, 54, 436, 4]\n",
      "[5]\n",
      "[15, 24, 8, 9, 26, 108, 27, 250, 4]\n",
      "[5]\n",
      "[942, 103, 111, 26, 109, 323, 26, 44, 250, 4]\n",
      "[5]\n",
      "[1004, 35, 893, 7, 38, 4]\n",
      "[5]\n",
      "[67, 13, 12, 9, 16, 900, 48, 299, 55, 42, 18, 31, 4]\n",
      "[5]\n",
      "[920, 407, 26, 78, 77, 65, 51, 55, 42, 18, 31, 4]\n",
      "[5]\n",
      "[195, 106, 310, 59, 9, 455, 86, 12, 15, 27, 31, 4]\n",
      "[5]\n",
      "[350, 9, 350, 9, 350, 12, 157, 74, 4]\n",
      "[5]\n",
      "[1710, 157, 8, 21, 54, 49, 4]\n",
      "[5]\n",
      "[96, 121, 216, 55, 42, 18, 36, 4]\n",
      "[5]\n",
      "[56, 28, 6763, 2610, 343, 50, 74, 4]\n",
      "[5]\n",
      "[15, 20, 664, 7, 38, 4]\n",
      "[5]\n",
      "[260, 62, 261, 52, 17, 19, 9, 681, 7, 80, 4]\n",
      "[5]\n",
      "[159, 20, 690, 97, 431, 9, 26, 15, 27, 31, 4]\n",
      "[5]\n",
      "[697, 28, 77, 920, 407, 26, 230, 250, 4]\n",
      "[5]\n",
      "[697, 35, 37, 7, 1233, 16, 145, 50, 74, 4]\n",
      "[5]\n",
      "[115, 147, 272, 33, 1638, 17, 19, 11, 4]\n",
      "[5]\n",
      "[1813, 2184, 11, 2]\n",
      "[5]\n",
      "[468, 1813, 2184, 57, 11, 4]\n",
      "[5]\n",
      "[159, 20, 36, 4]\n",
      "[5]\n",
      "[82, 65, 300, 26, 188, 17, 19, 11, 4]\n",
      "[5]\n",
      "[213, 12, 15, 24, 60, 23, 47, 4]\n",
      "[5]\n",
      "[175, 20, 217, 20, 2648, 76, 7, 44, 31, 4]\n",
      "[5]\n",
      "[174, 10, 2, 171, 562, 8, 14, 850, 8, 26, 485, 11, 4]\n",
      "[5]\n",
      "[368, 17, 19, 11, 4]\n",
      "[5]\n",
      "[5567, 10, 900, 48, 299, 55, 42, 18, 10, 16, 30, 4]\n",
      "[5]\n",
      "[1517, 1173, 33, 363, 9, 26, 15, 27, 31, 4]\n",
      "[5]\n",
      "[175, 327, 28, 1373, 7, 2155, 21, 2, 36, 4]\n",
      "[5]\n",
      "[3206, 12, 1052, 237, 7, 38, 4]\n",
      "[5]\n",
      "[56, 28, 6763, 485, 9, 293, 33, 50, 17, 19, 11, 4]\n",
      "[5]\n",
      "[475, 407, 26, 485, 19, 11, 4]\n",
      "[5]\n",
      "[56, 10, 142, 121, 27, 36, 4]\n",
      "[5]\n",
      "[1517, 1173, 33, 363, 11, 4]\n",
      "[5]\n",
      "[1339, 2387, 52, 7, 38, 4]\n",
      "[5]\n",
      "[992, 25, 169, 242, 2585, 24, 400, 38, 4]\n",
      "[5]\n",
      "[992, 25, 169, 242, 2585, 24, 400, 38, 4]\n",
      "[5]\n",
      "[144, 20, 108, 76, 201, 4]\n",
      "[5]\n",
      "[876, 42, 18, 554, 25, 283, 4]\n",
      "[5]\n",
      "[63, 12, 159, 55, 84, 12, 159, 55, 61, 8, 9, 45, 34, 49, 4]\n",
      "[5]\n",
      "Num of Sample: 59\n",
      "Total Score: 0.030473545995983745\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(transformer, enc_test, dec_test, mecab, mecab, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a8ee681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 25, 142, 121, 55, 84, 12, 18, 31, 4]\n",
      "[5]\n",
      "마음 도 정리 해야 할 때 가 있 어요 .\n",
      "[1509, 27, 31, 4, 475, 407, 21, 54, 20, 229, 7, 164, 61, 275, 11, 4]\n",
      "[5]\n",
      "두근거리 겠 어요 . 자연 스럽 지 않 은 곳 이 조금 만 기다리 세요 .\n"
     ]
    }
   ],
   "source": [
    "print(get_decoded_sentence(translate(get_encoded_sentence(mecab.morphs('지루하다, 놀러가고 싶어.'), word_to_index), transformer, mecab, mecab), index_to_word))\n",
    "print(get_decoded_sentence(translate(get_encoded_sentence(mecab.morphs('오늘 일찍 일어났더니 피곤하다.'), word_to_index), transformer, mecab, mecab), index_to_word))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
